{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Los Angeles Times on LAPD Misclassifying Crime Severity \n",
    "\n",
    "So, we've already seen this story with feature extraction and Logistic Regression earlier in the course. The story (which you can read [here](https://www.latimes.com/local/la-me-crimestats-lapd-20140810-story.html)) was reasonably straightforward: Over a period of one year, The LA Times found that 1200 violent crimes had been marked as minor offenses. \n",
    "\n",
    "Their approach—the same approach we looked at in class—was using keywords to extract _violent_ crimes based on what the FBI considered violence. These keywords were essentially the _features_ of the model trained. This is not bulletproof (let's face it, little is), and it takes a certain amount of manual effort to derive the features. You start with the FBI list (homicide, rape, robbery, aggravated assault, burglary, theft, motor vehicle theft, and arson), but then you have to augment that with other _features_. This includes, but is not limited to:\n",
    "\n",
    "- synonyms (e.g. rape: sexual assault; homicide: murder, manslaughter)\n",
    "- breakdown of assault weapons (knives, guns, types of guns, etc.)\n",
    "- breakdown of actions (\"shot\" for a gun crime, \"stab\" for a knife assault, etc.)\n",
    "\n",
    "Let's quickly go through how our earlier model performed, and then try to adopt some more text-centric machine learning approaches to tackle this same problem. Yes, there are many ways to skin the cat. Or, to extract features from text documents. Or, to tackle a machine learning problem. \n",
    "\n",
    "For this runthrough, we are simply going to use data from 2012 as opposed to the whole dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "# eliminates the pointless \"A value is trying to be set on a copy of a slice from a DataFrame\" warnings.\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data, and get a feel for it. \n",
    "df = pd.read_csv(\"data/2012-assaults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows do we have? How many columns? What are the columns we have? What do they look like?\n",
    "print(f\"The number of rows is {df.shape}[0] and the number of columns is {df.shape[1]}\")\n",
    "print(f\"The columns we have are: {df.columns}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so 39,000 datapoints and 15 _features_ or columns. But, we don't care about all the columns. For example, the `DR_NBR` or `OCC_DT` or `CRME_OCCUR_TM` don't contribute to the overall verdict of whether this crime was a \"simple\" assault or an \"aggravated\" assault. Or, in other words, irrespective of the date or time of the incident, the incident should be a \"simple\" assault or an \"aggravated\" assault.\n",
    "\n",
    "So, let's only focus on the columns we care about (i.e. the description and the narrative) and drop everything else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['DR_NBR', 'OCC_DT', 'OCC_TO_DT', 'CRME_OCCUR_TM', 'CRME_OCCUR_TO_TM', 'STR_NBR', 'STR_DIR_CD', 'STR_NM', 'STR_TYP_CD', 'CRS_STR_DIR_CD', 'CRS_STR_NM', 'CRS_STR_TYP_CD', 'NEW_CC_PRM'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this is better but still not great as we want to reduce each crime to Aggravated (also known as \"Part I\" crime) or Simple (also known as \"Part II crime\", which is comparatively less serious.) However, the `CCDESC` column seems to have multiple values, so, again, let's see what we're working with.\n",
    "\n",
    "But, also, some of the narrative columns seem to be empty. Let's see how many have NaN values, and filter those out, because without that data, we can't really make any classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {df.isna().sum()['DO_NARRATIVE']} NaN values in the `DO_NARRATIVE` column, which we will now drop.\")\n",
    "df = df.dropna(subset=['DO_NARRATIVE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CCDESC.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't end-of-the-world bad (few things are, let's be honest), so let's normalise our data, i.e. transform the above descriptions into two classes: \"Part I\" and \"Part II\". Remember: Part I is the more serious offense and Part II the comparatively minor one. \n",
    "\n",
    "So, we create a new column in our existing dataframe called `is_part_i` and check whether the description contains the word \"AGGRAVATED\". \n",
    "\n",
    "An aside: It's worth noting that we don't know how the dataset was created, i.e. from the LAPD point of view, is this freetext or do they have checkboxes or a dropdown they can use when they specify the nature of the crime. If it's checkboxes or a dropdown, we can be relatively sure that we aren't dealing with typos or spelling mistakes so we don't have to look at the data tooooo carefully. Or, to rephrase: currently, we're simply assuming \"AGGRAVATED\" is spelt correctly, and so we can run the below code. And, if we look at the `CCDESC.value_counts()`, we can see there aren't spelling mistakes. But, it's something that we should always double-check, because otherwise, things could fall through the cracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_part_i\"] = df.CCDESC.str.contains(\"AGGRAVATED\").astype(int)\n",
    "part_i_counts = df.is_part_i.value_counts()\n",
    "print(f\"The % of aggravated assaults in our dataset is {part_i_counts[1]/(part_i_counts[0] + part_i_counts[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle Data \n",
    "#### Manually Downgrading Part I Offenses\n",
    "The investigation by The Los Angeles Times uncovered Part I crimes that were being downgraded, and misclassified as Part II crimes.\n",
    "\n",
    "Since we don't have the original incorrect classifications, we're going to have to cheat a little. To reproduce a situation similar to the LA Times, let's take about 15 percent of our aggravated assaults and reclassify them as simple assaults. (This is simply to replicate the computational aspect of their investigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the official classification into a new column\n",
    "# We'll pretend this is what was reported by the police\n",
    "df['reported'] = df['is_part_i']\n",
    "# Now we'll downgrade a random 15% of the Part I crimes, changing their reported \"1\" (a Part I crime) status\n",
    "# to a 0 (not a Part I crime)\n",
    "downgraded_indices = df[df.is_part_i == 1].sample(frac=0.25).index\n",
    "df.loc[downgraded_indices, 'reported'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two columns: `reported` and `is_part_i` to see what our value counts for each are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------------------\")\n",
    "print(\"The breakdown of our reported column is:\")\n",
    "print(df.reported.value_counts())\n",
    "print(\"------------------------------------------\")\n",
    "print(\"The breakdown of our is_part_i column is:\")\n",
    "print(df.is_part_i.value_counts())\n",
    "print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Computers to Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the story, the LA Times journalists identified whether an assault was a \"Part I\" assault or \"Part II\" assault based on a custom list of words. Reporters searched the summaries for terms such as \"stab\" and \"knife\" to flag incidents that met the FBI criteria for serious offenses, and then read the summaries. They also reviewed court and police records for dozens of cases.\n",
    "\n",
    "As mentioned above, the problem with this approach is that it mandates the journalists come up with the list of **useful words**, and then read through the results. But, what about otehr situations that are less clear-cut to non-experts? Or, assaults which used other weapons? Say nunchucks or culverins or sabres or daggers? \n",
    "\n",
    "But what about other situations? Situations which might be less clear-cut to non-experts, or assaults involving non-traditional weapons? To wit, machetes make appearances in plenty of aggravated assaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.DO_NARRATIVE.str.contains(\"MACHETE\", na=False)].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've said in class before, one of the things you could do is talk to experts, and get a finite list of alternative words from them. Or, read through more than a handful of the descriptions to identify the more obscure aggravated assaults. \n",
    "\n",
    "Or, you know, let the machine learn for you. \n",
    "\n",
    "An algorithm can go through each of the narratives and find words that are likely to identify an assault as \"Part I\" or \"Part II\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising\n",
    "\n",
    "What is tokenisation anyway?\n",
    "\n",
    "One [definition](https://www.techopedia.com/definition/13698/tokenization): \"Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded. The tokens become the input for another process like parsing and text mining.\" \n",
    "\n",
    "So, tokenisation doesn't have to mean breaking up bigger texts into individual words. However, for this example, let's interpret tokenisation as the action of breaking up the narrative column into words, so that we can keep track of which words crop up in which situations. For example, `\"SUSP SWUNG UMBRELLA WITH METAL TIP AT V2\"` will result in a list comprising of:\n",
    "\n",
    "* SUSP\n",
    "* SWUNG\n",
    "* UMBRELLA\n",
    "* WITH\n",
    "* METAL\n",
    "* TIP\n",
    "* AT\n",
    "* V2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorising\n",
    "When we're talking about text analysis, vectorisation is the process by which we _translate_ 'tokens' to their IDs. IDs refer to a numeric representation of the word—the number itself is immaterial/unimportant. All that matters is that you create this translation or mapping. Every single time you deal with text, you'll have to vectorise it so that the computer can understand what's going on. \n",
    "\n",
    "Let's walk through a quick example, before we go back to our case study. \n",
    "\n",
    "#### Vectorisation: a trivial example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list of song titles that have the word 'wind' \n",
    "song_titles = [\n",
    "    \"The Wind Cries Mary\",\n",
    "    \"Wind of Change\", \n",
    "    \"Wind Beneath My Wings\",\n",
    "    \"Winding Road\", \n",
    "    \"The Long and Winding Road\", \n",
    "    \"A Pillow of Winds\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run through the basic not-very-smart vectorisation, and then let's make it smarter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "ft = cv.fit_transform(song_titles)\n",
    "ft.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when we put our data through the `CountVectorizer`, we end up with something like this, which, let's be honest, is completely unreadable. We'll pretty format it in a second, but let's try to get an intuition for what's going on. \n",
    "\n",
    "Our data has 16 unique words. The vectoriser has assigned each word a unique value between 0 and 15 (inclusive), and then gone through each of the song titles to see which word exists in which title. The resulting output is a _list of lists_ where each item in the list corresponds to a song title. Each list has 16 items in it: one item for each unique word in our dataset. Implicitly, the vectoriser has decided which word corresponds to which index in the list, i.e. the first item in the list is 'and', the second is 'beneath', the third is 'change' etc. And, then, it goes through the song title and assigns '1' if the word exists in that song and '0' if it doesn't. \n",
    "\n",
    "So, for example, the third song title is 'Wind Beneath My Wings' and the second item in the third row in the output above has a 1. Every other list has a '0' for that index, because no other song title contains the word 'Beneath'. This is similar to **one-hot encoding**, but not quite the same. \n",
    "\n",
    "The difference between this and one-hot encoding is that one-hot encoding indicates the _presence_ or _absence_ of a feature, whereas this indicates the _number of times_ a feature comes up. So, if we had an example where a word existed more than once in a song (say twice), the corresponding value would be '2', not 1. In one-hot encoding though, it would be 1. \n",
    "\n",
    "But, let's face it, that's completely unreadable, so let's try to actually make that human readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ft.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things about the above vectorisation:\n",
    "- the '1' indicates the word exists in that data item, the '0' indicates that it doesn't. So, for example, the 0th row in the dataframe above corresponds to the entry 'The Wind Cries Mary'. Only those four words have a '1' against them; the rest of the words have 0. \n",
    "- the output takes things like conjunctions and prepositions into account, which we don't need in _real_ analysis, i.e. the presence of an article or conjunction will not change the nature of a crime from major to minor. \n",
    "- the output doesn't differentiate between 'wind', 'winding', 'winds'. In different situations, you might want different things to happen. For example, you might want to conflate them together to get rid of unnecessary noise.  Or, if in the context of your dataset, they mean different things, you might want to keep them separate. \n",
    "\n",
    "Right, let's make this vectoriser somewhat smarter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(stemmer.stem(word) for word in analyzer(doc))\n",
    "\n",
    "\n",
    "tv = StemmedTfidfVectorizer(stop_words='english', min_df=1, max_df=1.0)\n",
    "ft = tv.fit_transform(song_titles)\n",
    "pd.DataFrame(ft.toarray(), columns=tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice a few things here:\n",
    "- all your conjunctions and prepositions have disappeared\n",
    "- all variations of the word 'wind' are now conflated into one _feature_\n",
    "- ...and some of the _feature names_ look odd: 'chang', 'cri', 'mari'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorising the LAPD dataset\n",
    "\n",
    "We do **exactly** the same thing as we did above with the toy example. Copy and paste the code, but just change the variable name(s). \n",
    "\n",
    "Let's start with the not-so-smart vectoriser to see what we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "ft = cv.fit_transform(df.DO_NARRATIVE)\n",
    "word_appearances = pd.DataFrame(ft.toarray(), columns=cv.get_feature_names()).head()\n",
    "word_appearances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 14,000 features? That's way too much effort; let's make this a tad easier for ourselves. For one, a lot of the words we have here are unhelpful. They might be too specific to a crime, or they might be a typo/spelling mistake. For example, below we might be looking at postcodes, street numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_appearances.columns[30:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, if we do something like this, we are doing exactly the same thing as above, i.e. creating a finite list of words to check against. Yep, we get to know that those words exist in the narrative, but this isn't really much better than the 'search for a specific set of words' approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_appearances[['knife', 'argument', 'handgun', 'wrestle', 'pushed', 'injuries', 'fired', 'irate', 'umbrella']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving on to the slightly smarter vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc:(stemmer.stem(word) for word in analyzer(doc))\n",
    "\n",
    "tv = StemmedTfidfVectorizer(stop_words='english', min_df=5, max_df=0.5)\n",
    "ft = tv.fit_transform(df.DO_NARRATIVE)\n",
    "word_appearances = pd.DataFrame(ft.toarray(), columns=tv.get_feature_names())\n",
    "word_appearances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, OK, this is better than our previous result, i.e. from 14,000 features, we're now down to ~2500. \n",
    "\n",
    "Before we go any further, let's talk about what the parameters to our `StemmedTfidfVectorizer` mean. \n",
    "\n",
    "- `stop_words` is a list of words that are very common in a language, and so can be dropped from the vectorisation process. Most NLP libraries have these lists built in, and they drop words like 'and', 'the', 'in', etc.\n",
    "- `min_df` is the `minimum document frequency`, i.e. the minimum number of documents the term has to appear in to be considered useful. For example, if a word appears just once in our dataset, it might not tell us anything meaningful. Now, that's not to say you won't lose out on an obscure weapon, but that will very likely be very very very obscure. Here, we specify the number 5. \n",
    "- `max_df` is the `maximum document frequency`, i.e. the maximum number of documents the word should appear in. Notice that this is a decimal. This means 50 percent, i.e. the word should only appear in half the documents or less. Why? Because if it's more prevalent than that in our dataset, it's probably there for _all_ crimes, and not just violent crimes. \n",
    "\n",
    "`sklearn` treats integers as number of documents and decimals as fraction of documents in this case, i.e. for both `min_df` and `max_df` you can specify either an integer or a decimal, and `sklearn` will figure out what you intend to do accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Now that we have the narratives **vectorized**, i.e. they're now in a format that the computer can understand, we can teach our machine which words to associate with which type of crimes.\n",
    "\n",
    "Just like a human being, the computer will go through each sentence, seeing which words are usually found in a \"Part I\" crime and which are found in a \"Part II\" crime, and how many times these words appear in the narrative. Accordingly, the rows will be classified. \n",
    "\n",
    "We'll start by using a Random Forest, which is just one among many different machine learning techniques that we've seen thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "training = df\n",
    "X = tv.fit_transform(training.DO_NARRATIVE)\n",
    "y = training.reported\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we are teaching our model with the **\"fake\" downgraded categories**. That's because in the real world we wouldn't know which ones are accurately reported and which ones aren't. Let's see if we have ample data for our model to perform reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "\n",
    "Now that our model has read through ~40,000 narratives, it should have a decent idea of what an aggravated assault vs. a simple assault is. We can test how good it is with a couple fake sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X = tv.transform([\n",
    "  \"S SHOT AND STABBED V WITH A GUN AND THE GUN HAD A KNIFE ON IT\",\n",
    "  \"SUSP STRUCK VICTS NOSE\",\n",
    "  \"S PUNCHED THEIR NEIGHBOR\"\n",
    "])\n",
    "clf.predict(sample_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting array above indicates that `1` was predicted for the first sentence and `0` for the second, which seems reasonable, as `1` is a violent crime and `0` is a less violent one.\n",
    "\n",
    "Note: see how we use `tv.transform(...)` here instead of `tv.fit_transform(...)` as we did above? This is because we want the new words to be transformed to our existing vectoriser, i.e. the words in these sentences should map to the corresponding IDs of our training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Misclassified Offenses\n",
    "\n",
    "The real task at hand, however, is to use this to find potentially misclassified cases. \n",
    "\n",
    "To do so, we will show our model all our case narratives, and let it predict whether each one should be a Part I or Part II crime. More violent ones should be predicted as `1`, and less violent ones will be predicted as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tv.transform(df.DO_NARRATIVE)\n",
    "df['prediction'] = clf.predict(X)\n",
    "df.prediction.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that ~6,000 crimes have been identified as violent and ~25,000 as not. Now, let's see where the mismatch lies, i.e. which crimes were reported as non-violent but predicted to be violent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.reported == 0) & (df.prediction == 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about if we specifically only look at the crimes we **downgraded**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.is_part_i == 1) & (df.reported == 0)].prediction.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is *pretty terrible*. From the Part I crimes that were downgraded to Part II, only about 8% were correctly identified as Part I. OK, this kind-of sucks, but at least we can try another classifier to see if that's better suited for this kind of problem. \n",
    "\n",
    "Logistic Regression to the rescue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1e9)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "df['prediction'] = clf.predict(X)\n",
    "df[(df.is_part_i == 1) & (df.reported == 0)].prediction.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we go from 149 being correctly identified to 888. We still have over a thousand misclassified though, so while we've improved on `RandomForest`, we're still not nearly anywhere near good enough. \n",
    "\n",
    "So, let's try another classifier. This is a new one called `LinearSVC`, which is part of the `SVM` or `support vector machine` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Make our predictions and see how it looks\n",
    "df['prediction'] = clf.predict(X)\n",
    "df[(df.is_part_i == 1) & (df.reported == 0)].prediction.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, while we still have over a thousand misclassified, we've one-upped Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, this is something new you haven't seen yet. You could have a look at the `sklearn` documentation of this [here](https://scikit-learn.org/stable/modules/svm.html#svm-classification). But, intuitively, what is support vector machine? \n",
    "In the simplest possible terms, it creates a linear classifier which tries to maximise the margin between the training points and the decision boundary. Or, well, because a picture speaks a thousand words: ![title](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fwp-content%2Fuploads%2F2015%2F10%2FSVM_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image, there are two classes: stars and circles. We can have many different decision boundaries (lines A, B, C are just three examples), where everything to the left of the decision boundary would be classified as a star and everything to the right of the decision boundary line would be classified as a circle. \n",
    "\n",
    "When you optimise to maximise the margin, you get the best classifier, which in our case is `C`. What does maximising the margin mean? Well, effectively it decides the classifier that gives you the best accuracy while being furthest away from the training data points on both sides. In the above example, the accuracy for A, B, and C are the same, but C is furthest away from the training data points on both: the stars and the circles. [Alternatively](https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html): a decision boundary drawn in the middle of the void between data items of the two classes seems better than one which approaches very close to examples of one or both classes.\n",
    "\n",
    "So, why is it called `Support Vectors`? (don't you love the nomenclature?)\n",
    "\n",
    "Well, because the decision boundary—the margin or the separator—is decided by a very small subset of the data/by a very few points. This small subset is referred to as \"support vectors\" (as in a vector space, a point can be thought of as a vector between the origin and that point). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
