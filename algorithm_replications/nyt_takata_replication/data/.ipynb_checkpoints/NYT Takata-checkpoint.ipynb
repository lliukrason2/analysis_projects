{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faulty Takata Airbags using Logistic Regression\n",
    "\n",
    "**The story:**\n",
    "- https://www.nytimes.com/2014/09/12/business/air-bag-flaw-long-known-led-to-recalls.html\n",
    "- https://www.nytimes.com/2014/11/07/business/airbag-maker-takata-is-said-to-have-conducted-secret-tests.html\n",
    "- https://www.nytimes.com/interactive/2015/06/22/business/international/takata-airbag-recall-list.html\n",
    "- https://www.nytimes.com/2016/08/27/business/takata-airbag-recall-crisis.html\n",
    "\n",
    "This story, done by The New York Times, investigates the content in complaints made to National Highway Traffic Safety Administration (NHTSA) by customers who had bad experiences with Takata airbags in their cars. Eventually, car companies had to recall airbags made by the airbag supplier that promised a cheaper alternative. \n",
    "\n",
    "**Author:** Daeil Kim did a more complex version of this particular analysis - [presentation here](https://www.slideshare.net/mortardata/daeil-kim-at-the-nyc-data-science-meetup)\n",
    "\n",
    "**Topics:** Logistic Classifier\n",
    "\n",
    "**Datasets**\n",
    "\n",
    "* **FLAT_CMPL.txt:** Vehicle-related complaints from 1995-current from the [National Highway Traffic Safety Administration](https://www-odi.nhtsa.dot.gov/downloads/)\n",
    "* **CMPL.txt:** data dictionary for the above\n",
    "* **sampled-unlabeled.csv:** a sample of vehicle complaints, not labeled\n",
    "* **sampled-labeled.csv:** a sample of vehicle complaints, labeled with being suspicious or not\n",
    "\n",
    "## What's the goal?\n",
    "\n",
    "It's too much work to read twenty years of vehicle comments to find the ones related to dangerous airbags! Because we're lazy, we want the computer to do this for us. We're going to read a subset, mark each one as \"suspicious\" or \"not suspicious,\" then use that information to train the computer to read the rest and recognize which comments are suspicious and which are not suspicious.\n",
    "\n",
    "This is a **classification** problem, because we want the computer to recognize which ones are suspicious and which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our code\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Allow us to display 100 columns at a time, and 100 characters in each column (instead of ...)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in our data\n",
    "\n",
    "The dataset in `FLAT_CMPL.txt` doesn't have column headers, so we're going to use this long long list of headers that we stole from `CMPL.txt` to read it in.\n",
    "\n",
    "It's kind of a complicated dataset with a few errors here or there, so we're passing in a *lot* of options to `pd.read_csv`. In the end it's just a big big dataframe, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['CMPLID', 'ODINO', 'MFR_NAME', 'MAKETXT', 'MODELTXT', \n",
    "                'YEARTXT', 'CRASH', 'FAILDATE', 'FIRE', 'INJURED', \n",
    "                'DEATHS', 'COMPDESC', 'CITY', 'STATE', 'VIN', 'DATEA', \n",
    "                'LDATE', 'MILES', 'OCCURENCES', 'CDESCR', 'CMPL_TYPE', \n",
    "                'POLICE_RPT_YN', 'PURCH_DT', 'ORIG_OWNER_YN', 'ANTI_BRAKES_YN', \n",
    "                'CRUISE_CONT_YN', 'NUM_CYLS', 'DRIVE_TRAIN', 'FUEL_SYS', 'FUEL_TYPE', \n",
    "                'TRANS_TYPE', 'VEH_SPEED', 'DOT', 'TIRE_SIZE', 'LOC_OF_TIRE', \n",
    "                'TIRE_FAIL_TYPE', 'ORIG_EQUIP_YN', 'MANUF_DT', 'SEAT_TYPE', \n",
    "                'RESTRAINT_TYPE', 'DEALER_NAME', 'DEALER_TEL', 'DEALER_CITY', \n",
    "                'DEALER_STATE', 'DEALER_ZIP', 'PROD_TYPE', 'REPAIRED_YN', \n",
    "                'MEDICAL_ATTN', 'VEHICLES_TOWED_YN']\n",
    "\n",
    "df = pd.read_csv(\"FLAT_CMPL.txt\",\n",
    "                 sep='\\t',\n",
    "                 dtype='str',\n",
    "                 header=None,\n",
    "                 error_bad_lines=False,\n",
    "                 encoding='latin-1',\n",
    "                 names=column_names)\n",
    "\n",
    "# We're only interested in pre-2015\n",
    "df = df[df.DATEA < '2015']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows and columns are in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But wait, we don't even need that yet\n",
    "\n",
    "Oof, that's a lot of columns!\n",
    "\n",
    "When you're dealing with machine learning, one of the first things you'll need to think about is what columns are important to you. An important thing about this dataset is **it doesn't include whether the complaint is about faulty airbags or not.**\n",
    "\n",
    "We can't teach our classifier what a suspicious comment looks like if we don't have a list of suspicious complaints, right? Luckily, we have another dataset of labeled complaints!\n",
    "\n",
    "**Read in `sampled-labeled.csv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use this dataset to **train our classifier about what a suspicious complaint looks like.** Once our classifier is trained we'll be able to use it to predict whether each complaint in that original (big big big) dataset is suspicious or not.\n",
    "\n",
    "We made this dataset through hard work, reading comments, and marking them as `0` (not suspicious) or `1` (suspicious). For example, this complaint isn’t suspicious because it’s about an air bag _not_ deploying:\n",
    "\n",
    "```\n",
    "DURING AN  ACCIDENT  AIR BAG'S DID NOT DEPLOY.  DEALER HAS BEEN CONTACTED.  *AK  \n",
    "```\n",
    "\n",
    "This next one isn’t suspicious either, because it isn’t even about airbags!\n",
    "\n",
    "```\n",
    "DRIVERS SEAT BACK COLLAPSED AND BENT WHEN REAR ENDED. PLEASE DESCRIBE DETAILS.  TT\n",
    "```\n",
    "\n",
    "But if something involves explosions or shrapnel happens, it’s probably worth marking as suspicious:\n",
    "\n",
    "```I WAS DRIVEN IN A SCHOOL ZONE STREET AND THE LIGHTS OF AIRBAG ON AND APROX. 2 MINUTES THE AIR BAGS EXPLODED IN MY FACE, THE DRIVE AND PASSENGERS SIDE, THEN I STOPPED THE JEEP, IT SMELL LIKE SOMETHING IS BURNING AND HOT, I DID NOT SEE FIRE.  *TR\n",
    "```\n",
    "\n",
    "So we went down the file in Excel, one by one, reading comments, marking them as 0 or 1.\n",
    "\n",
    "**How many are in each category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 non-suspicious and 15 suspicious is a pretty terrible ratio, but we're remarkably lazy and not very many of the comments are actually suspicious.\n",
    "\n",
    "Now that we've read a few, let's train our classifier\n",
    "\n",
    "## Creating features\n",
    "\n",
    "When you're working on machine learning, you need to feed the algorithm a bunch of inputs so it can make its decision. These are called **features**.\n",
    "\n",
    "There's a problem: computers only like features to be numbers, but every complaint is **just a bunch of text**, a.k.a. \"unstructured data.\" How can we turn all of this unstructured data into something a computer can understand?\n",
    "\n",
    "While there are fancier (and more effective!) ways to do what we're about to do, the simple start below is going to provide a foundation for later work.\n",
    "\n",
    "To teach our computer how to find suspicious complaints, we first need to think about how we find those complaints as human beings. By reading, right? So let's teach the computer how to read, and what to look for.\n",
    "\n",
    "### Designing our features\n",
    "\n",
    "Let's take a look at what the airbag issue is, according [Consumer Reports](https://www.consumerreports.org/car-recalls-defects/takata-airbag-recall-everything-you-need-to-know/):\n",
    "\n",
    "> Vehicles made by 19 different automakers have been recalled to replace frontal airbags on the driver’s side or passenger’s side, or both in what NHTSA has called \"the largest and most complex safety recall in U.S. history.\" The airbags, made by major parts supplier Takata, were mostly installed in cars from model year 2002 through 2015. Some of those airbags could deploy explosively, injuring or even killing car occupants. \n",
    "> \n",
    "> At the heart of the problem is the airbag’s inflator, a metal cartridge loaded with propellant wafers, which in some cases has ignited with explosive force. If the inflator housing ruptures in a crash, metal shards from the airbag can be sprayed throughout the passenger cabin—a potentially disastrous outcome from a supposedly life-saving device.\n",
    "\n",
    "If we're going through a list of vehicle complaints, it isn't too hard for us to figure out which complaints we might want to investigate further. If the complaint's about seatbelts or rear-view mirrors, we probably don't care about it. If the word \"airbag\" shows up in the description, though, we're going to start paying attention.\n",
    "\n",
    "We aren't interested in all complaints with the word \"airbag,\" though. Since we're worried about exploding airbags, something like \"the airbag did not deploy\" would get our attention because of the word \"airbag,\" but then we could ignore it once we saw the airbag just didn't work.\n",
    "\n",
    "### Selecting our features\n",
    "\n",
    "Since we just read a long long list of airbag complaints, we can probably brainstorm some words or phrases that might make a comment interesting or not interesting. A quick start might be these few:\n",
    "\n",
    "* airbag\n",
    "* air bag\n",
    "* failed\n",
    "* did not deploy\n",
    "* violent\n",
    "* explode\n",
    "* shrapnel\n",
    "\n",
    "These **features** are the things that the machine learning algorithm is going to look for when it's reading. There are lots of words in each complaint, but these are the only ones we'll tell the classifier to pay attention to!\n",
    "\n",
    "### Building our features dataframe\n",
    "\n",
    "Now we're going to convert each sentence into a list of numbers. It will be a new dataframe, where there's a `1` if the word is in the complaint and a `0` if it isn't.\n",
    "\n",
    "To determine if a word is in `CDESCR`, we can use `.str.contains`.\n",
    "\n",
    "**See if each row has the word `AIRBAG` in it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers can't use `True` and `False`, though, we need numbers. We'll need to use `.astype(int)` to turn them into intgers, with `0` for `False` and `1` for `True`.\n",
    "\n",
    "**Give me a `1` for every row that contains \"AIRBAG\" and a `0` fo every row that does not.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many `0` values and how many `1` values do we have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so about 200 don't have `AIRBAG` mentioned and about 150 do. That's a decent balance, I guess!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make a new dataframe with a row for each complaint. Each word will have a column, and we'll have `0` or `1` as to whether the word is in there or not.\n",
    "\n",
    "* airbag\n",
    "* air bag\n",
    "* failed\n",
    "* did not deploy\n",
    "* violent\n",
    "* explode\n",
    "* shrapnel\n",
    "\n",
    "Along with the words, we'll **also save the `is_suspicious` label** to keep everything in the same place.\n",
    "\n",
    "I've started the dataset with the label and the word **airbag**, you'll need to add in the rest of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'is_suspicious': labeled.is_suspicious,\n",
    "    'airbag': labeled.CDESCR.str.contains(\"AIRBAG\", na=False).astype(int),\n",
    "})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many rows and columns your dataframe has. You'll want to make sure it has **8 columns**, and they should all be numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The kind of problem we're dealing with here is called a **classification problem**. That's because we have two different classes of complaints:\n",
    "\n",
    "* Complaints that are suspicious\n",
    "* Complaints that are not suspicious\n",
    "\n",
    "And the machine's job is to classify new complaints in one of those two categories. Before we put it on the job, though, we need to **train it**.\n",
    "\n",
    "Before we start with that, though, let's see how many suspicious and non-suspicious comments are in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a second, I thought we had 350 rows? Where are the rest?\n",
    "\n",
    "* **Tip:** Try adding `dropna=False` to your `.value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, it looks like we're missing a LOT of labels. Classifiers hate missing data - both missing labels _and_ missing features - so we might as well remove any row that's missing any data.\n",
    "\n",
    "* **Tip:** If you use `.dropna()`, it will drop any rows that have `NaN` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the missing rows, double-check that your dataframe is the size you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our classifier\n",
    "\n",
    "Just like with linear regression, we call our classifier a **model**. It **models** the relationship between the inputs and the outputs.\n",
    "\n",
    "The classifier we're using is a special one that uses **logistic regression** under the hood, but that doesn't matter very much right now. Just know that it's a classifier!\n",
    "\n",
    "### Separating our features and labels\n",
    "\n",
    "We need to feed our classifier two things\n",
    "\n",
    "1. The features\n",
    "2. The labels\n",
    "\n",
    "Take a look at the first five rows of `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is_suspicious` is our label, and all of the othe columns are our features. We'll call the label `y` and the features `X`, because that's what everyone else does.\n",
    "\n",
    "The typical way of doing it is below (many people might use `axis=1` instead of `columns=`, but I like how explicit `columns=` is!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that .drop doesn't drop the column permanently, it only drops the column to save it into `X`\n",
    "X = train_df.drop(columns=['is_suspicious'])\n",
    "y = train_df.is_suspicious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at `X` and `y` to make sure they look like a list of features and a list of labels. You can use `.head()` on both of them, no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our classifier\n",
    "\n",
    "One we have our features and our labels, we can create a classifier.\n",
    "\n",
    "I'm actually going to move the `X=` and `y=` down into this section because it's nice to keep it all in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Every column EXCEPT whether it's suspicious\n",
    "X = train_df.drop(columns='is_suspicious')\n",
    "# label is suspicious 0/1\n",
    "y = train_df.is_suspicious\n",
    "\n",
    "# Build a new classifier\n",
    "# C=1e9 is a magic secret I don't want to talk about\n",
    "# If we don't say solver='lbfgs' it complains that it's the new default\n",
    "clf = LogisticRegression(C=1e9, solver='lbfgs')\n",
    "\n",
    "# Teach the classifier about the complaints we read\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that... seems to have done nothing.\n",
    "\n",
    "When we do linear regression, it prints out a bunch of stuff for us. It's nice! When we train a classifier, **it's up to us to use the classifier.**\n",
    "\n",
    "## Interpreting our classifier\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "So the classifier did some reading. Hooray! We gave it all sorts of columns (each was a different word)... which columns did it think were important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words we were looking for,\n",
    "# X were our features, X.columns is the column names\n",
    "feature_names = X.columns\n",
    "\n",
    "# Coefficients! Remember this from linear regression?\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "}).sort_values(by='coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher number for a coefficient means \"this word makes me think it's suspicious, a.k.a. `1`\" and a lower number means \"this word makes me think it was not suspicious, a.k.a. `0`.\"\n",
    "\n",
    "Is there anything you found surprising about these results? Why do you think that might have happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with our classifier\n",
    "\n",
    "The point of a classifier is to classify documents it hasn't seen before, to read them and put them into the appropriate category. Before we can do this, we need to **extract features from our original dataframe**, the one that doesn't have labels.\n",
    "\n",
    "We'll do this the **same way** we did with our set of labeled data. Build a new dataframe that asks whether each complaint has the appropriate word:\n",
    "\n",
    "* airbag\n",
    "* air bag\n",
    "* failed\n",
    "* did not deploy\n",
    "* violent\n",
    "* explode\n",
    "* shrapnel\n",
    "\n",
    "I've started you off with one check for the word **airbag**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame({\n",
    "    'airbag': df.CDESCR.str.contains(\"AIRBAG\", na=False).astype(int),\n",
    "})\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe should have 7 columns, **none of which are `is_suspicious`**. It's unlabeled, remember? We aren't sure whether they're suspicious complaints or not.\n",
    "\n",
    "Confirm that real quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add a new column, the classifier's guess about whether it's suspicious or not. To make the classifier guess, we use `.predict`. We just feed our features to the classifier and there we go!\n",
    "\n",
    "* **Tip:** Use `clf.predict(features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a copy of `features` and give it a new column called `predicted`. That way if we need to use features again we won't have messed it up by adding new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_prediction = features.copy()\n",
    "features_with_prediction['predicted'] = clf.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty boring, right? No words in there, all predicted as `0`, not fun at all. Let's try filtering to see **the first ten where the prediction was `1`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the ones marked as suspicious include the words \"airbag\" and \"violent,\" and none of them include \"failed\" or \"did not deploy.\" That all makes sense, but what about all of the ones that include the word \"violent\" but not \"airbag\" or \"air bag?\" None of those should be good!\n",
    "\n",
    "While we could just filter it to only include ones with the word \"airabg\" in it, we probably need a way to **test the quality of our classifier**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our classifier\n",
    "\n",
    "When we look at the results of our classifier, we know some of them are wrong - complaints shouldn't be suspicious if they don't have airbags in them! But it would be nice to have an **automated process** to give us an idea of how well our classifier does.\n",
    "\n",
    "The problem is **we can't test our classifier on this unlabeled data**, because it doesn't know what's right and what's wrong. Instead, we have to test on the **labeled data** we trained our classifier on.\n",
    "\n",
    "One technique would be having our classifier compare the actual labels on our training data to what it would predict those labels to be.\n",
    "\n",
    "* **Tip:** Use `clf.score(X, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at our training data, predict the labels,\n",
    "# then compare the labels to the actual labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incredible, over 90% accuracy! ...that's good, right? **Well, not really.** There are two major reason why this isn't impressive!\n",
    "\n",
    "### Test-train split\n",
    "\n",
    "One big problem with our classifier is that we're testing it on **data it's already seen**. While it's cool to have a study sheet for a test, it doesn't quite seem fair if the **study sheet is exactly the same as the test**.\n",
    "\n",
    "Instead, we should try to reproduce what the real world is like - trainig it on one set of data, and testing it on *similar* data... but similar data we already know the labels for!\n",
    "\n",
    "To make this happen we use something called **train/test split**, where instead of using the _entire_ dataset for training, we only use _most_ of it - the default is 80% for training and 20% for testing. The code on the line below automatically splits the dataset into two groups, one for training and a smaller one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to understand what's going on, take a look at `X_train`, `X_test`, `y_train` and `y_test`, along with their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `X_` and the `y_` variables look just about exactly the same, the only difference is that `_train` contains a lot more than `_test`, and there are no repeats between the two.\n",
    "\n",
    "Now when we give the model a test, it hasn't seen the answers already!\n",
    "\n",
    "* Use `clf.fit` to train on the training sample\n",
    "* Use `clf.score` to score on the testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is fun, because there's a chance *it will get even better!* Weird, right? We'll talk about why that might have happened a little later.\n",
    "\n",
    "There are other ways to improve this further, but for now we have a larger problem to tackle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix\n",
    "\n",
    "Our accuracy is looking great, hovering somewhere in the 90's. Feeling good, right? **Unfortunately, things aren't actually that rosy.**\n",
    "\n",
    "Let's take a look at how many suspicious and how many non-suspicious ones we have in our labeled dataset (for the millionth time, yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot more non-suspicious ones as compared to suspicious, right? Let's say we were classifying, and we *always* guessed \"not suspicious\". Since there are so few suspicious ones, we wouldn't get very many wrong, and our accuracy would be really high!\n",
    "\n",
    "> If we have 99 non-suspicious and 1 suspicious, if we always guess \"non-suspicious\" we'd have 99% accuracy.\n",
    "\n",
    "Even though our accuracy would look great, the result would be super boring. Since zero of our complaints would have been marked as suspicious, we wouldn't have anything to read or research. **It'd be much nicer if we could identify the difference between getting one category right compared to the other.**\n",
    "\n",
    "And hey, that's easy! We use this thing called a **confusion matrix**. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = y\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which is pretty terrible-looking, right? It's hard as heck to understand! Let's try to spice it up a little bit and make it a little nicer to read:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Save the true label, but also save the predicted label\n",
    "y_true = y\n",
    "y_pred = clf.predict(X)\n",
    "# We could also use just the test dataset\n",
    "# y_true = y_test\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# But then make it look nice\n",
    "label_names = pd.Series(['not suspicious', 'suspicious'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can see what's going on a little bit better. According to the confusion matrix, when using our original dataset (your numbers might be a little different):\n",
    "\n",
    "* We correctly predicted 149 of 150 not-suspicious\n",
    "* We only correctly predicted 2 of 15 suspicious ones.\n",
    "\n",
    "Even though that gives us a really high score, **it's pretty useless**.\n",
    "\n",
    "## Thinking about what your outputs mean\n",
    "\n",
    "While we could spend a lot of time working on the math behind all of this and the technical ins and outs, I think a more useful thing for journalists to do - when both analyzing their own algorithms as well as other people's algorithms - is to think about **what incorrect outputs mean**.\n",
    "\n",
    "In this case, we're trying to predict whether we should investigate a given complaint. That basically means, \"the computer takes a look and says 'hey human being, you should go look at it'.\n",
    "\n",
    "As a result, every complain that _shouldn't_ have been flagged is more work for a computer, but every complaint that is _incorrectly_ flagged means we'll never think to look at that complaint.\n",
    "\n",
    "**Do you think it's better to incorrectly flag non-suspicious complaints as suspicious, or to incorrectly flag suspicious complaints as non-suspicious**\n",
    "\n",
    "What are the upsides/downsides of each, and which side is more important to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Probability\n",
    "\n",
    "When we use `clf.predict`, we only get a `0` or a `1`. That's kind of a fakeout, though, as under the hood there is actually something a (little) more complicated going on. Since we only have two categories, each row is given a score between 0-100% as to whether it should belong to a category. If it's over 50% it goes into that category!\n",
    "\n",
    "We can see this with `clf.predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_predictions = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_predictions['predicted'] = clf.predict(X)\n",
    "# [:,1] is the probability it belongs in the '1' category\n",
    "X_with_predictions['probability'] = clf.predict_proba(X)[:,1]\n",
    "X_with_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can be a little more discriminating - instead of just looking as whether it scored above or below 50% by seeing the final classification we can see exactly what the classifier was thinking when it assigned it to one category or another. Try sorting by probability and showing the top 20, putting the higher probability at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's improve our model\n",
    "\n",
    "Right now our model isn't very good. It doesn't seem to require the word \"airbag\" to be in it (maybe because we count \"airbag\" and \"air bag\" as separate words?) and doesn't include that many features. Can you think of ways to improve our model, and maybe try a few out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We'll just do this all over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.set_option(\"display.max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our labeled data\n",
    "\n",
    "Right now we're only dropping ones have missing labels. Why do we have so many missing labels? Are there other options for ones we could include/not include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data, drop those that are missing labels\n",
    "labeled = pd.read_csv(\"data/sampled-labeled.csv\")\n",
    "labeled = labeled.dropna()\n",
    "labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our X and y\n",
    "\n",
    "Are there other words you might look for? Any words you might remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'is_suspicious': labeled.is_suspicious,\n",
    "    'airbag': labeled.CDESCR.str.contains(\"AIRBAG\", na=False).astype(int),\n",
    "    'air bag': labeled.CDESCR.str.contains(\"AIR BAG\", na=False).astype(int),\n",
    "    'failed': labeled.CDESCR.str.contains(\"FAILED\", na=False).astype(int),\n",
    "    'did not deploy': labeled.CDESCR.str.contains(\"DID NOT DEPLOY\", na=False).astype(int),\n",
    "    'violent': labeled.CDESCR.str.contains(\"VIOLENT\", na=False).astype(int),\n",
    "    'explode': labeled.CDESCR.str.contains(\"EXPLODE\", na=False).astype(int),\n",
    "    'shrapnel': labeled.CDESCR.str.contains(\"SHRAPNEL\", na=False).astype(int),\n",
    "})\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test\n",
    "\n",
    "Does giving the model more (or less) to train with change anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns='is_suspicious')\n",
    "y = train_df.is_suspicious\n",
    "\n",
    "# With test_size=0.3, we'll train on 70% and test on 30%\n",
    "# random_state=42 means it isn't actually random, it will always give you the same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train our classifier\n",
    "\n",
    "You... don't know any other classifiers. But hey, you could always look some up, I guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1e9, solver='lbfgs')\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the important words\n",
    "\n",
    "Are the selected words pushing your results in the direction you think they should?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "# Coefficients! Remember this from linear regression?\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "}).sort_values(by='coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our classifier\n",
    "\n",
    "We'll do a simple `.score` (which we know isn't very useful) along with a confusion matrix (which is harder to understand, but less useful). How do we feel about the results according to both?\n",
    "\n",
    "**Normally I'd only use the confusion matrix on `X_test`/`y_test`, but we do such a bad job that I feel like we should look at it all.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y\n",
    "y_pred = clf.predict(X)\n",
    "# y_true = y_test\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "label_names = pd.Series(['not suspicious', 'suspicious'])\n",
    "pd.DataFrame(matrix,\n",
    "     columns='Predicted ' + label_names,\n",
    "     index='Is ' + label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you keep running this and running this, it's going to be different each time.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_predictions = train_df.copy()\n",
    "train_df_with_predictions['predicted'] = clf.predict(train_df.drop(columns='is_suspicious'))\n",
    "train_df_with_predictions['predicted_prob'] = clf.predict_proba(train_df.drop(columns='is_suspicious'))[:,1]\n",
    "train_df_with_predictions['sentence'] = labeled.CDESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_predictions.sort_values(by='predicted_prob', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are we going to fix this?\n",
    "\n",
    "Even if you can't successfully make your classifier perform any better, try to think about what you feel like could make it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
