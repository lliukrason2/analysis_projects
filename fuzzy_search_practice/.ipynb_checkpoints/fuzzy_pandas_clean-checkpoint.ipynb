{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching and Fuzzy Pandas\n",
    "      \n",
    "[Max Harlow](https://twitter.com/maxharlow), a journalist at the Financial Times, wrote this library `csvmatch`, and he's been adding new algorithms to facilitate fuzzy matching across datasets. He's used it for a bunch of stories, including:\n",
    "- https://www.theguardian.com/uk-news/2014/jul/09/offshore-tax-dealings-celebrities-sportsmen-leaked-jersey-files\n",
    "- https://www.theguardian.com/politics/2014/jul/08/offshore-secrets-wealthy-political-donors\n",
    "\n",
    "Similar techniques have also been used in other stories like:\n",
    "- https://www.globalwitness.org/en/campaigns/oil-gas-and-mining/myanmarjade\n",
    "- https://www.irinnews.org/investigation/2016/09/02/exclusive-un-paying-blacklisted-diamond-company-central-african-republic\n",
    "\n",
    "But, wait, first: \n",
    "\n",
    "### What is Fuzzy Matching? \n",
    "\n",
    "Automating the look-up for names in documents is [inherently imprecise](https://www.elastic.co/blog/found-fuzzy-search). The computer can't _know_ that different representations of the same _thing_ refer to the same _thing_. For example: \n",
    "- _Apple Inc._; _Apple Computer Company_; _Apple Computer, Inc._; and _Apple_ all refer to the fruit company. \n",
    "- _Samuel Langhorne Clemens_, _Samuel L. Clemens_, _Samuel Clemens_ and Mark Twain all refer to the same person. \n",
    "- _Robert Ford_, _Rob Ford_, and _Robert Frod_ refer to the same person **probably**. \n",
    "\n",
    "When you're working with unstructured data, you can't take anything for granted. Least of all, you can't assume that:\n",
    "- documents will have correct spellings\n",
    "- first, last, and middle names will exist in all documents\n",
    "- the abbreviated/shortened names of people won't make an appearance (e.g. Jon instead of Jonathan, Tom instead of Thomas, Phil instead of Philip, etc.) \n",
    "\n",
    "So, when you're living in an uncertain world, you try to make things slightly more _certain_ with **Fuzzy Matching**. You might not hit 100 percent, but at least you'll hit more than what you would without fuzzy matching. \n",
    "\n",
    "There are multiple algorithms that try to minimise the uncertainty/enable fuzzy matching. The library we are going to be look at today incorporates a bunch of these, instead of just doing one thing. \n",
    "\n",
    "This notebook's predominantly based on an [awesome NICAR2019 presentation](https://docs.google.com/presentation/d/1djKgqFbkYDM8fdczFhnEJLwapzmt4RLuEjXkJZpKves/) where Max Harlow (the aforementioned news app developer at the Financial Times) demonstrated [csvmatch](https://github.com/maxharlow/csvmatch). And, then, Soma basically created a library to make it with Pandas. \n",
    "\n",
    "Worth remembering that there are no shortcuts in life, and few panaceas. Depending on the project you're working on, you might be more inclined to use one algorithm or the other. Or, you know, try a few of them and see what happens. And, also, remember: all computational tools you use need to hand-in-hand with traditional reporting. People share names, there's more than one John Smith, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you `pip install fuzzy_pandas` first. \n",
    "\n",
    "import pandas as pd\n",
    "import fuzzy_pandas as fpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Toy Example\n",
    "\n",
    "We'll be working with two toy datasets first, just to get going and get an idea as to what's possible. The names of the files are not terribly imaginative: `data1.csv` and `data2.csv`. And, they both contain structured data: names, code names and locations of characters from John le Carré's spy thriller: Tinker Tailor Soldier Spy. \n",
    "\n",
    "Right, let's have a look. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/data1.csv\")\n",
    "df2 = pd.read_csv(\"data/data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact matches\n",
    "\n",
    "We start with doing \"exact matches\", i.e. both tables should have the exact same name. Capitalisation matters, accents matter. With this function, for example:\n",
    "- John le Carre will not match with John le Carré\n",
    "- George SMILEY will not match with George Smiley\n",
    "\n",
    "Based on what you see in the data frames above, how many matches do you expect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so, we only find one match as expected. But, are there any other matches that a _smarter_ algorithm could find? Let's try something called **Levenshtein**, a nifty simple algorithm that's pretty common. It's the basis for a bunch of spellcheck algorithms, amongst other things, and the way it works is it checks the number of characters that are different between two inputs, and if the _distance_ is small enough, it assumes the two words are the same. \n",
    "\n",
    "For example, in the above two data frames, you have Toby Esterhase and Tony Esterhase, which means the Levenshtein distance is 1 (The 'b' v. 'n' in To(b,n)y.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing you'll notice above is that, by default, the _Levenshtein_ algorithm doesn't care about case. \n",
    "\n",
    "However, are we still missing potential matches? \n",
    "\n",
    "When we work with any algorithms, we need a confidence threshold that we decide on. By default, the `csvmatch` algorithm has a `threshold` of 0.6, i.e. only if the algorithm returns a match score greater than or equal to 0.6 will it return a match. \n",
    "\n",
    "The score, in this case, is calculated using the below formula: \n",
    "\n",
    "> `1 - (distance/maximum(value1, value2))`\n",
    "\n",
    "We can be slightly more conservative with the threshold, and we get a Brand New Result in our output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is _cool_. By changing the threshold, we found another match based on what the pronunciation of the two names are: Connie and Konny. **But, what could be cooler?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **metaphone** algorithm does phonetic matching, and gives you results based on that. \n",
    "\n",
    "Note: In theory, the documentation says that you can combine a couple of these algorithms if you're so inclined. But, it looks like when you combine two algorithms, it doesn't _quite_ work. ¯\\_(ツ)_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## swap the methods around and then look at the results, too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think is happening here? \n",
    "\n",
    "This is important—you're often going to be using tools built by other folks, but where there's code, there are bugs. You should make sure that you play with the tool a bit to make sure it's doing _exactly_ what you think it's doing. And, if it's not, you know where it falls short. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less Fictional Datasets\n",
    "\n",
    "We are going to be using the same datasets Max Harlow used for this exercise. As he explains in his presentation [here](https://docs.google.com/presentation/d/1djKgqFbkYDM8fdczFhnEJLwapzmt4RLuEjXkJZpKves/edit#slide=id.g3512a0ce6b_1_22), there are a bunch of files: \n",
    "- a list of world billionaires published by Bloomberg\n",
    "- a similar list published by Forbes\n",
    "- a list also published by Forbes that only includes Chinese individuals\n",
    "- a list published by the CIA of chiefs of state and cabinet members of foreign governments\n",
    "- a list of all the people that attended the World Economic Forum conference in Davos this year\n",
    "- a list of all the people and companies that have been sanctioned by the United Nations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the two billionaire lists (Forbes + Bloomberg)\n",
    "\n",
    "df1 = pd.read_csv(\"data/forbes-billionaires.csv\")\n",
    "df2 = pd.read_csv(\"data/bloomberg-billionaires.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find out how many billionaires appear in both lists (exact matching)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find the ones where the ranks aren't the same across the two datasets? What about the ones that are the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy matching with non-fictional data\n",
    "\n",
    "In the above couple of cells, we've conducted \"exact matching\", i.e. the equivalent of you running a `Cmd+F`/`Ctrl+F` on your text editor. But, this is _almost_ worse as it's case sensitive, i.e. \"Tom\" and \"tom\" are treated differently. \n",
    "\n",
    "We've gone through some of this already, but what are the things we can ignore when it comes to name-matching? Harlow, in his presentation, identified:\n",
    "- case\n",
    "- title (Mr., Mrs., etc.)\n",
    "- non-latin characters (é, å, ß, etc.)\n",
    "- the order of the names\n",
    "- non-alphanumerics (e.g. hyphenated names)\n",
    "\n",
    "Now, you don't _have to_ ignore _anything_, but sometimes, it might make your life far easier. Other times, you'll end up with false positives and whatnot. \n",
    "\n",
    "The library `csvmatch`—and by extension `fuzzy_pandas`—support a bunch of the above parameters, which you can just pass in to the function. Passing in a bunch of these parameters would allow you to go from `Orbán, Viktor` to `Viktor Orban`, which is quite useful. (Again, the example's from Harlow's slides)\n",
    "\n",
    "For this bit, we'll move on to two of the other datasets: `cia-world-leaders.csv` and `davos-attendees-2019.csv`. As always, read in the data and figure out which columns the exact match should run on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cia_world_leaders = pd.read_csv('data/cia-world-leaders.csv')\n",
    "davos_attendees = pd.read_csv('data/davos-attendees-2019.csv')\n",
    "print(f\"Our CIA World Leaders df has these columns: {cia_world_leaders.columns} \\\n",
    "      \\n The Davos attendees have these: {davos_attendees.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cia_world_leaders.sort_values('name').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davos_attendees.sort_values('full_name').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by seeing what an exact match would look like. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there are no matches. What do you reckon? What _should_ the overlap between CIA's list of world leaders and Davos attendees be? \n",
    "\n",
    "Let's try some _fuzzy matching_, first by simply ignoring case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have more matches, but this is also pretty boring. There's nothing super-smart about ignoring cases to get matches. Your word processors have been doing that for _decades_. \n",
    "\n",
    "But, now, let's start adding some of our other parameters discussed above, and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, 19 more results. Baby steps, but at least steps in the right direction. Now, let's start using the more _intelligent_ algorithms in place—this one named after a Russian mathematician: Levenshtein. \n",
    "\n",
    "All *Levenshtein* does is look at how many characters are different between two inputs? For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jellyfish._jellyfish import damerau_levenshtein_distance\n",
    "\n",
    "damerau_levenshtein_distance(\"Évry\", \"Every\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly use the algorithm directly to see the output. The above cell imports something called `jellyfish`, which is another package that `csvmatch` uses. Typically, you wouldn't call the function directly (you could if you wanted to), but this is just to give you an idea of how the algorithm works. \n",
    "\n",
    "Right, now let's use this with our above data, and see if we have better luck. \n",
    "\n",
    "Remember: the threshold specified by us is 0.6, and it's calculated by: `1-(distance/max(value1, value2))`. In the case of `Évry` and `Every` above, our calculation would be:\n",
    "\n",
    "`1 - (2/5)` = `3/5` = `0.6`\n",
    "\n",
    "So, in this case, the two would lead to a fuzzy match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAIT, WHAT?!** Have we just gone from 138 matches to 1952? Is that overtly optimistic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why, yes. Yes, it is. This is why you _always_ confirm what an algorithm does. Right, maybe let's bump up our threshold and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that seems more reasonable. Let's sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question, one for you guys to do:\n",
    "\n",
    "Which names from the CIA world leaders list are also on the Forbes billionaires list?\n",
    "\n",
    "Who can get the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in what scenarios do you reckon Levenshtein will perform badly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: **metaphone**. \n",
    "\n",
    "Metaphone's great for names which sound similar, which wouldn't be caught by Levenshtein. It's especially handy when you're working with transcript data. But, it too comes with its pitfalls. \n",
    "\n",
    "Let's look at an example and then discuss what the possible pitfalls could be. \n",
    "\n",
    "Which names from the CIA world leaders list are also on the United Nations sanctions list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_sanctions = pd.read_csv(\"data/un-sanctions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How does this compare to our other algorithms (ignore case)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How does this compare to our other algorithms (levenshtein)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to **Bilenko**. And, yes, this uses machine learning, where you train your own data. So, now you have human *smarts* being involved in the process of matching up names across documents. \n",
    "\n",
    "Let's look at an example: \n",
    "\n",
    "Which names from the CIA world leaders list are also on the Davos attendees list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
